from llama_cpp import Llama
from typing import List, Dict, Any
from datetime import datetime
import json
import re

MODEL_PATH = r"D:\AI\Models\gemma-3n-E4B-it-Q4_K_M.gguf"
N_THREADS = 4

SYSTEM_PROMPT_RCA = """
Il tuo nome Ã¨ Simix. Sei un assistente AI specializzato in analisi delle cause radice (RCA) per una linea di produzione di moduli fotovoltaici.

La tua funzione principale Ã¨ guidare lâ€™utente nellâ€™identificare la causa radice di un problema attraverso la tecnica dei 5 PerchÃ©.

â€”

ðŸ“¦ CONTESTO TECNICO DELLA FABBRICA:
La linea produttiva Ã¨ composta da queste stazioni principali:
1. GLASS Loader â€“ carico del vetro
2. STR (Stringer) â€“ saldatura delle celle
3. AIN (Bussing) â€“ saldatura delle interconnessioni
4. MIN (Quality Gate) â€“ controllo qualitÃ  manuale
5. MBJ â€“ applicazione della junction box
6. RMI â€“ Rework (rilavorazione)
7. LMN (Laminatore) â€“ incapsulamento
8. VPF (Visual Inspection) â€“ ispezione finale

I moduli avanzano tramite rulliere e buffer intermedi. Ogni modulo Ã¨ associato a una stazione, un operatore, un tempo ciclo, ed eventuali difetti (NG).

Difetti comuni:
- NG1 â€“ rottura vetro
- NG2 â€“ disallineamento celle
- NG3 â€“ saldatura mancante
- NG4 â€“ errore etichetta

â€”

ðŸ“Œ ISTRUZIONI:
Devi SEMPRE rispondere **solo** in formato JSON, senza testo aggiuntivo o simboli.
Segui esattamente questo schema (senza mai aggiungere <|file_separator|> o testo extra):

{
  "question": "PerchÃ© [testo della domanda]?",
  "suggestions": [
    "risposta 1",
    "risposta 2",
    "risposta 3"
  ]
}
"""

# Inizializza il modello
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=4096,
    n_threads=N_THREADS,
    n_batch=256,
    use_mlock=False,
    use_mmap=True,
    verbose=False
)

def build_prompt_with_suggestions(case_context: str, why_chain: List[Dict[str, str]]) -> str:
    prompt = f"<|system|>\n{SYSTEM_PROMPT_RCA.strip()}\n<|user|>\n"
    prompt += f"Contesto: {case_context.strip()}\n\n"
    if why_chain:
        prompt += "Domande e risposte finora:\n"
        for idx, step in enumerate(why_chain, start=1):
            prompt += f"{idx}. Q: {step['q']}\n   A: {step['a']}\n"
        prompt += "\nFornisci la **prossima domanda e almeno 3 possibili risposte** in formato JSON.\n"
    else:
        prompt += "Inizia con la **prima domanda e almeno 3 possibili risposte** in formato JSON.\n"
    prompt += "<|assistant|>\n"
    return prompt

def ask_next_why_with_suggestions(case_context: str, why_chain: List[Dict[str, str]]) -> Dict[str, Any]:
    prompt = build_prompt_with_suggestions(case_context, why_chain)
    response_stream = llm(prompt, max_tokens=512, stop=["<|user|>"], stream=True)

    raw_text = ""
    print("ðŸ¤– Simix (streaming):\n")
    for chunk in response_stream:
        if isinstance(chunk, dict):
            choices = chunk.get("choices")
            if choices and isinstance(choices, list) and "text" in choices[0]:
                token = choices[0]["text"]
                raw_text += token
                print(token, end="", flush=True)
        elif isinstance(chunk, str):
            raw_text += chunk
            print(chunk, end="", flush=True)
    print("\n")

    # Clean output (remove garbage like file separators and markdown fences)
    raw_text = raw_text.replace("<|file_separator|>", "").strip()
    # Remove Markdown ```json ... ``` wrappers if present
    raw_text = re.sub(r"^```(?:json)?", "", raw_text, flags=re.IGNORECASE).strip()
    raw_text = re.sub(r"```$", "", raw_text).strip()

    # Try to parse JSON
    try:
        # Fix potential trailing commas
        raw_text = re.sub(r",\s*]", "]", raw_text)
        raw_text = re.sub(r",\s*}", "}", raw_text)
        data = json.loads(raw_text)
        question = data.get("question", "").strip()
        suggestions = [s.strip() for s in data.get("suggestions", [])]
    except Exception:
        question = "Errore nel parsing della domanda."
        suggestions = []


    return {"question": question, "suggestions": suggestions}

def run_rca_chat():
    print("\nðŸ§  Avvio Simix RCA (5 Why) con suggerimenti AI (output JSON)\n")
    case_context = input("ðŸ“Œ Inserisci contesto (es: NG2 su STR01, modulo #1234...):\n> ").strip()
    why_chain = []

    while True:
        print("\nðŸŸ¦ Simix sta pensando alla prossima domanda e suggerimenti...\n")
        result = ask_next_why_with_suggestions(case_context, why_chain)
        next_q = result["question"]
        suggestions = result["suggestions"]

        print(f"ðŸ”µ {next_q}")
        if suggestions:
            print("ðŸ“‹ Suggerimenti AI:")
            for i, s in enumerate(suggestions, start=1):
                print(f"  {i}. {s}")
        
        user_input = input("ðŸŸ¢ Scrivi la risposta oppure digita il numero del suggerimento (Enter per uscire):\n> ").strip()

        if user_input == "":
            break
        elif user_input.isdigit() and 1 <= int(user_input) <= len(suggestions):
            selected = suggestions[int(user_input) - 1]
            print(f"âœ… Hai selezionato: {selected}")
            why_chain.append({"q": next_q, "a": selected})
        else:
            why_chain.append({"q": next_q, "a": user_input})

    print("\nâœ… Catena completa 5 Why:\n")
    for idx, step in enumerate(why_chain, start=1):
        print(f"{idx}. {step['q']}")
        print(f"   â†³ {step['a']}")
    print("\nðŸ’¾ (In futuro: salveremo questa RCA nel database.)")

if __name__ == "__main__":
    run_rca_chat()
