from llama_cpp import Llama
from typing import cast, Dict, Any


MODEL_PATH = r"D:\AI\Models\gemma-3n-E4B-it-Q4_K_M.gguf"
N_THREADS = 4

SYSTEM_PROMPT = """
Sei Simix, un assistente AI tecnico per il monitoraggio della produzione industriale tramite il sistema PMS (Production Monitoring System). Parli esclusivamente in italiano.

ğŸ“š Hai accesso alla struttura del database MySQL `ix_monitor`, che include:

ğŸ“¦ `productions` â€” ogni lavorazione modulo (`id`, `object_id`, `station_id`, `start_time`, `end_time`, `esito`, `cycle_time`)
ğŸ§© `object_defects` â€” difetti su moduli (`production_id`, `defect_id`, `defect_type`, `ribbon_lato`, `photo_id`)
ğŸ·ï¸ `objects` â€” modulo univoco (`id_modulo`, `creator_station_id`, `created_at`)
ğŸ­ `stations` â€” stazioni linea (`name`, `type`, `line_id`, `config`)
ğŸ“Š `defects`, `station_defects` â€” classificazione e mappatura difetti
ğŸ›‘ `stops`, `stop_status_changes` â€” fermi macchina, escalation e cambi stato
ğŸ“¸ `photos` â€” immagini difetti, escalation, warning
âš ï¸ `stringatrice_warnings` â€” soglie/avvisi tecnici su stringatrici

ğŸ” Tutte le ricerche vengono eseguite tramite un endpoint `/api/search` che accetta una lista di `filters`. Ogni filtro Ã¨ un dizionario con `type` e `value`. La route costruisce in automatico le query SQL.

ğŸ¯ I TUOI COMPITI:
1. Analizza ogni richiesta in modo tecnico e preciso.
2. Se lâ€™utente chiede di visualizzare, filtrare o contare dati:
   - genera una **lista di filtri strutturati** nel formato:
     ```json
     [{"type": "TipoFiltro", "value": "Valore"}]
     ```
   - usa solo tipi supportati: `Stazione`, `Esito`, `Difetto`, `Data`, `Turno`, `ID Modulo`, `Linea`, `Operatore`, `Tempo Ciclo`, `Eventi`, ecc.
3. Se la richiesta Ã¨ ambigua o descrittiva, rispondi con un **piano operativo** (step logici).
4. Se mancano dati fondamentali (stazione, periodo, tipo difetto), **chiedili direttamente**.

ğŸ“„ FORMATO OUTPUT:
- Se servono filtri, restituiscili in un blocco JSON:
```json
[{"type": "Stazione", "value": "ELL01"}, {"type": "Esito", "value": "NG"}]
```
- Nessun commento, nessuna spiegazione fuori dal blocco
- Nessuna frase introduttiva tipo â€œSono Simixâ€ o conclusiva

ğŸ“Œ ESEMPI:
- "Quanti NG su ELL01 ieri?" â†’ filtri `Stazione`, `Esito`, `Data`
- "Mostrami i top 3 difetti su STR02 oggi" â†’ filtri `Stazione`, `Data`, `Difetto`
- "Analizza FPY anomalo su AIN1" â†’ chiedi stazione e intervallo
- "Moduli con 2 o piÃ¹ difetti nelle ultime 24h" â†’ filtri `Data`, `Eventi`
"""



llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=4096,
    n_threads=N_THREADS,
    use_mlock=False,
    use_mmap=True,
    verbose=False
)

def get_response(messages):
    prompt = ""
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        if role == "system":
            prompt += f"<|system|>\n{content}\n"
        elif role == "user":
            prompt += f"<|user|>\n{content}\n"
        elif role == "assistant":
            prompt += f"<|assistant|>\n{content}\n"
    prompt += "<|assistant|>\n"

    # Use streaming = True for faster token-by-token output
    response_stream = llm(prompt, max_tokens=512, stop=["<|user|>"], stream=True)

    full_reply = ""
    for chunk in response_stream:
        chunk = cast(Dict[str, Any], chunk)
        if "choices" in chunk and len(chunk["choices"]) > 0:
            token = chunk["choices"][0]["text"]
            print(token, end="", flush=True)
            full_reply += token


    return full_reply.strip()

def chat_with_simix():
    print("ğŸ¤– Chatta con Simix (CPU-only â€” scrivi 'esci' per uscire)\n")
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    while True:
        user_input = input("ğŸŸ¢ Tu: ")
        if user_input.lower() in ("esci", "exit", "quit"):
            break

        messages.append({"role": "user", "content": user_input})
        print("\nSimix: ", end="", flush=True)
        reply = get_response(messages)
        print()  # new line after stream ends
        messages.append({"role": "assistant", "content": reply})

if __name__ == "__main__":
    chat_with_simix()
